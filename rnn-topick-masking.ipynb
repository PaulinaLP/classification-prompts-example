{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":2622,"sourceType":"modelInstanceVersion","modelInstanceId":1899},{"sourceId":2938,"sourceType":"modelInstanceVersion","modelInstanceId":2180}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import concurrent.futures\nfrom collections import Counter\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom nltk import sent_tokenize, word_tokenize\nfrom itertools import chain\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T07:22:52.506317Z","iopub.execute_input":"2024-05-31T07:22:52.506623Z","iopub.status.idle":"2024-05-31T07:23:14.154848Z","shell.execute_reply.started":"2024-05-31T07:22:52.506595Z","shell.execute_reply":"2024-05-31T07:23:14.153818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = hub.KerasLayer(\"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\")\nencoder = hub.KerasLayer(\"https://kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-10-h-128-a-2/2\", trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:26:29.093886Z","iopub.execute_input":"2024-05-31T07:26:29.094257Z","iopub.status.idle":"2024-05-31T07:26:41.523402Z","shell.execute_reply.started":"2024-05-31T07:26:29.094226Z","shell.execute_reply":"2024-05-31T07:26:41.522633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global variables","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\"\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nOUTPUT_PATH = '/kaggle/working/'\n\n# Model parameters:\n# as this is a simple model we use small vocabulary size and truncate to 400 words\nNUM_EPOCHS = 100\nVOCABULARY_SIZE = 10000\nEMBEDDING_DIM = 120\nMAX_LENGTH = 4000\nTRUNCATION_TYPE = 'post'\nPADDING_TYPE = 'pre'\nOOV_TOKEN = '<OOV>'\nTRAIN_SIZE = .75\nPATIENCE = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:28:05.009713Z","iopub.execute_input":"2024-05-31T07:28:05.010344Z","iopub.status.idle":"2024-05-31T07:28:05.015734Z","shell.execute_reply.started":"2024-05-31T07:28:05.010312Z","shell.execute_reply":"2024-05-31T07:28:05.014712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# that are the 7 topics of the essays and the corresponding leading tf idf terms\nvenus=['venus']\nmars=['mars','face','alien','aliens','landform','martian','mesa']\ncars=['accidents','self','smart','cars','car','driverless','driveless','usage','sensors','drive','driver','transportation','vehicle','vehicles','driving']\nelectoral=['vote','electoral','president','electors', 'state', 'senator','collage','ellection']\ntechnology=['happy','huang','muscles','mona','lisa','computer','software','technology','emotions','facial','emotion','student','students','kids','teachers','feeling','teacher','feel','expressions']\nluke=['luke','cowboy', 'seagoing','seagoing','cowboys','program','animals']\npollution=['air','pollution','smog','emissions','environment','city','paris','cities']\ntopic_words = venus + mars + cars + electoral +technology + luke + pollution","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:28:11.363634Z","iopub.execute_input":"2024-05-31T07:28:11.364708Z","iopub.status.idle":"2024-05-31T07:28:11.371998Z","shell.execute_reply.started":"2024-05-31T07:28:11.364670Z","shell.execute_reply":"2024-05-31T07:28:11.370921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:28:13.977135Z","iopub.execute_input":"2024-05-31T07:28:13.977531Z","iopub.status.idle":"2024-05-31T07:28:14.811520Z","shell.execute_reply.started":"2024-05-31T07:28:13.977499Z","shell.execute_reply":"2024-05-31T07:28:14.810503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding all words similar to known topic words","metadata":{}},{"cell_type":"markdown","source":"First I will find all the different words that appear at least 3 times across all texts.","metadata":{}},{"cell_type":"code","source":"all_text = ' '.join(df_train['full_text'])\n# Tokenize the text into words, convert to lowercase, and extract unique words\nwords = re.findall(r'\\b\\w+\\b', all_text.lower())\nword_counts = Counter(words)\n# Filter words that occur at least three times\nwords_used_at_least_three_times = [word for word, count in word_counts.items() if count >= 3]\nprint(f'Words used at least three times: {len(words_used_at_least_three_times)}')\nunique_words = set(words_used_at_least_three_times)\nvocab = list(unique_words)\nvocab = sorted(vocab, key=lambda word: word_counts[word], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:28:20.701645Z","iopub.execute_input":"2024-05-31T07:28:20.701996Z","iopub.status.idle":"2024-05-31T07:28:25.443723Z","shell.execute_reply.started":"2024-05-31T07:28:20.701967Z","shell.execute_reply":"2024-05-31T07:28:25.442767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I create embeddings for known topic words and for all frequent words. And I compare the vectors to find words that are similar to topic words.\n\nAfter trying different thresholds I decided to choose 0.984 of similarity. ","metadata":{}},{"cell_type":"code","source":"# as the vocabulary is more than 20 K we need to calculate embeddings in batches\ndef get_bert_embedding(word, preprocessor, encoder):\n    text_input = tf.constant([word])\n    encoder_inputs = preprocessor(text_input)\n    outputs = encoder(encoder_inputs)\n    pooled_output = outputs[\"pooled_output\"]  # [batch_size, 128]\n    return pooled_output.numpy()\n\n# Get embeddings for topic words\ntopic_embeddings = {word: get_bert_embedding(word, preprocessor, encoder) for word in topic_words}\n\n# Get embeddings for the vocabulary\ndef process_batch(words):\n    embeddings = {}\n    for word in words:\n        embeddings[word] = get_bert_embedding(word, preprocessor, encoder)\n    return embeddings\n\n# Split the vocabulary into batches\nbatch_size = 1000\nending=len(vocab)\n#for testing\n#ending=500\nbatches = [vocab[i:i+batch_size] for i in range(0, ending, batch_size)]\n\n# Process batches in parallel\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = executor.map(process_batch, batches)\n\n# Combine embeddings from all batches\nvocab_embeddings = {}\nbatch_count = 0\nfor result in results:\n    batch_count += 1\n    print(f\"Processed batch {batch_count} out of {len(batches)}\")\n    vocab_embeddings.update(result)\nprint (2)\n\n# Function to find similar words based on cosine similarity\ndef get_similar_words(topic_embeddings, vocab_embeddings, threshold=0.8):\n    similar_words = set()\n    for topic_word, topic_emb in topic_embeddings.items():\n        for vocab_word, vocab_emb in vocab_embeddings.items():\n            if topic_word != vocab_word:\n                similarity = cosine_similarity(topic_emb, vocab_emb)[0][0]\n                if similarity > threshold:\n                    similar_words.add(vocab_word)\n    return similar_words\n\n# Find similar words (for testing different thresholds)\n#for tr in np.arange(0.98,0.99,0.002):\n#    related_words = get_similar_words(topic_embeddings, vocab_embeddings, threshold=tr)\n#    print(f'{tr} Related Words: {len(related_words)}')\n#    print (related_words)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:28:31.648704Z","iopub.execute_input":"2024-05-31T07:28:31.649327Z","iopub.status.idle":"2024-05-31T07:31:02.377275Z","shell.execute_reply.started":"2024-05-31T07:28:31.649294Z","shell.execute_reply":"2024-05-31T07:31:02.376321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"related_words = get_similar_words(topic_embeddings, vocab_embeddings, threshold=0.984)\n# Combine topic words and related words\nall_exclude_words = set(topic_words).union(related_words)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:34:55.560444Z","iopub.execute_input":"2024-05-31T07:34:55.561323Z","iopub.status.idle":"2024-05-31T07:40:34.101520Z","shell.execute_reply.started":"2024-05-31T07:34:55.561275Z","shell.execute_reply":"2024-05-31T07:40:34.100702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the column with text in the df I change all topic words and all words similar to those for word \"topic\". The objective is to concentrate on the structure of the essays rather then on topics and make it easier for RNN to train the model.","metadata":{}},{"cell_type":"code","source":"def exclude_words(text, words):\n    pattern = re.compile(r'\\b(' + '|'.join(words) + r')\\b', re.IGNORECASE)\n    return pattern.sub(\"topic\", text)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:45:44.784149Z","iopub.execute_input":"2024-05-31T07:45:44.784995Z","iopub.status.idle":"2024-05-31T07:45:44.790332Z","shell.execute_reply.started":"2024-05-31T07:45:44.784964Z","shell.execute_reply":"2024-05-31T07:45:44.789448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['txt_wth_topic']=df_train['full_text'].apply(lambda x : exclude_words(x,all_exclude_words))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:45:47.177484Z","iopub.execute_input":"2024-05-31T07:45:47.178364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN Model to predict the score","metadata":{}},{"cell_type":"code","source":"print (\"running the model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:57:06.399416Z","iopub.execute_input":"2024-05-31T07:57:06.400094Z","iopub.status.idle":"2024-05-31T07:57:06.404519Z","shell.execute_reply.started":"2024-05-31T07:57:06.400062Z","shell.execute_reply":"2024-05-31T07:57:06.403643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the data\ndf = df_train.sample(frac=1).reset_index(drop=True)\n# Select the columns with the features\ndf_data_x = df[['txt_wth_topic']]\n# Select the column with the label:\ndf_data_y = df['score'].astype(int)\n\n# Split the samples:\nx_train, x_test, y_train, y_test = train_test_split(df_data_x,\n                                                    df_data_y,\n                                                    train_size=TRAIN_SIZE,\n                                                    random_state=42)\n\n# create tokenizer\ntokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token=OOV_TOKEN)\ntokenizer.fit_on_texts(x_train['txt_wth_topic'])  \ntrain_sequences = tokenizer.texts_to_sequences(x_train['txt_wth_topic'])\n\n# padding / truncating\ntrain_padded = pad_sequences(train_sequences,\n                             maxlen=MAX_LENGTH,\n                             padding=PADDING_TYPE,\n                             truncating=TRUNCATION_TYPE)\n\nvalidation_sequences = tokenizer.texts_to_sequences(x_test['txt_wth_topic'])\nvalidation_padded = pad_sequences(validation_sequences,\n                                  maxlen=MAX_LENGTH,\n                                  padding=PADDING_TYPE,\n                                  truncating=TRUNCATION_TYPE)\n\n# encoding\nencoder = LabelEncoder()\nencoder.fit(y_train)    \nencoded_y_train = to_categorical(encoder.transform(y_train))\nencoded_y_test = to_categorical(encoder.transform(y_test))\nnum_classes = len(np.unique(y_train))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:57:11.549045Z","iopub.execute_input":"2024-05-31T07:57:11.549706Z","iopub.status.idle":"2024-05-31T07:57:20.537430Z","shell.execute_reply.started":"2024-05-31T07:57:11.549663Z","shell.execute_reply":"2024-05-31T07:57:20.536517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\ntf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_DIM),\ntf.keras.layers.Dropout(0.4),\ntf.keras.layers.LSTM(128, return_sequences=True),\ntf.keras.layers.LSTM(64),\ntf.keras.layers.Dense(256, activation=\"relu\"),\ntf.keras.layers.Dense(128, activation=\"relu\"),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(1, activation=\"linear\")\n]) \n\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n\n# Build the model by providing the input shape\nmodel.build(input_shape=(None, MAX_LENGTH))\n\n# summary and callback\nmodel.summary()\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n\n# training\nhistory = model.fit(train_padded,\n                    y_train,\n                    epochs=NUM_EPOCHS,\n                    validation_data=(validation_padded, y_test),\n                    callbacks=[callback]\n                    ) \n\npredictotron = model.predict(validation_padded) ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:57:24.090485Z","iopub.execute_input":"2024-05-31T07:57:24.091308Z","iopub.status.idle":"2024-05-31T08:11:17.785123Z","shell.execute_reply.started":"2024-05-31T07:57:24.091274Z","shell.execute_reply":"2024-05-31T08:11:17.784271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing","metadata":{}},{"cell_type":"code","source":"def printing (chosen_history):\n    model_history = pd.DataFrame(chosen_history.history)\n    model_history['epoch'] = chosen_history.epoch\n\n    fig, ax = plt.subplots(1, figsize=(8, 6))\n    num_epochs = model_history.shape[0]\n\n    ax.plot(np.arange(0, num_epochs), model_history[\"mse\"], label=\"Training mse\")\n    ax.plot(np.arange(0, num_epochs), model_history[\"val_mse\"], label=\"Validation mse\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    fig, ax = plt.subplots(1, figsize=(8, 6))\n    num_epochs = model_history.shape[0]\n\n    ax.plot(np.arange(0, num_epochs), model_history[\"loss\"], label=\"Training loss\")\n    ax.plot(np.arange(0, num_epochs), model_history[\"val_loss\"], label=\"Validation loss\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:23:11.390636Z","iopub.execute_input":"2024-05-31T08:23:11.391004Z","iopub.status.idle":"2024-05-31T08:23:11.399595Z","shell.execute_reply.started":"2024-05-31T08:23:11.390974Z","shell.execute_reply":"2024-05-31T08:23:11.398659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printing(history)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:23:17.293369Z","iopub.execute_input":"2024-05-31T08:23:17.294233Z","iopub.status.idle":"2024-05-31T08:23:18.055102Z","shell.execute_reply.started":"2024-05-31T08:23:17.294181Z","shell.execute_reply":"2024-05-31T08:23:18.053899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(TEST_DATA_PATH)\ndf_test['txt_wth_topic']=df_test['full_text'].apply(lambda x : exclude_words(x,all_exclude_words))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:23:28.352945Z","iopub.execute_input":"2024-05-31T08:23:28.353787Z","iopub.status.idle":"2024-05-31T08:23:28.498673Z","shell.execute_reply.started":"2024-05-31T08:23:28.353751Z","shell.execute_reply":"2024-05-31T08:23:28.497885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"def predict (df_to_predict, chosen_model):\n    df_sequences = tokenizer.texts_to_sequences(df_to_predict['txt_wth_topic'])\n    df_sequences = pad_sequences(df_sequences,\n                                     maxlen=MAX_LENGTH,\n                                     padding=PADDING_TYPE,\n                                     truncating=TRUNCATION_TYPE)\n\n    prediction = chosen_model.predict(df_sequences)\n    prediction=prediction.clip(1, 6).round()\n    prediction=prediction.astype(int)\n    return prediction","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:27:10.928601Z","iopub.execute_input":"2024-05-31T08:27:10.929399Z","iopub.status.idle":"2024-05-31T08:27:10.934741Z","shell.execute_reply.started":"2024-05-31T08:27:10.929363Z","shell.execute_reply":"2024-05-31T08:27:10.933846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_test = predict(df_test,model)\nprint(prediction_test[:,0])\npredictions_df = pd.DataFrame({'essay_id': df_test['essay_id'],\n                               'score': prediction_test[:,0]})\nprint(predictions_df)\n\npredictions_df.to_csv(os.path.join(OUTPUT_PATH, \"submission.csv\"), index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:30:34.469138Z","iopub.execute_input":"2024-05-31T08:30:34.469947Z","iopub.status.idle":"2024-05-31T08:30:34.713874Z","shell.execute_reply.started":"2024-05-31T08:30:34.469912Z","shell.execute_reply":"2024-05-31T08:30:34.712781Z"},"trusted":true},"execution_count":null,"outputs":[]}]}