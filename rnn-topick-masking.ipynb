{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import concurrent.futures\nfrom collections import Counter\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom nltk import sent_tokenize, word_tokenize\nfrom itertools import chain\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = hub.KerasLayer(\"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\")\nencoder = hub.KerasLayer(\"https://kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-10-h-128-a-2/2\", trainable=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global variables","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\"\nOUTPUT_PATH = '/kaggle/working/'\n\n# Model parameters:\n# as this is a simple model we use small vocabulary size and truncate to 400 words\nNUM_EPOCHS = 100\nVOCABULARY_SIZE = 2000\nEMBEDDING_DIM = 120\nMAX_LENGTH = 400\nTRUNCATION_TYPE = 'post'\nPADDING_TYPE = 'pre'\nOOV_TOKEN = '<OOV>'\nTRAIN_SIZE = .75\nPATIENCE = 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# that are the 7 topics of the essays and the corresponding leading tf idf terms\nvenus=['venus']\nmars=['mars','face','alien','aliens','landform','martian','mesa']\ncars=['accidents','self','smart','cars','car','driverless','driveless','usage','sensors','drive','driver','transportation','vehicle','vehicles','driving']\nelectoral=['vote','electoral','president','electors', 'state', 'senator','collage','ellection']\ntechnology=['happy','huang','muscles','mona','lisa','computer','software','technology','emotions','facial','emotion','student','students','kids','teachers','feeling','teacher','feel','expressions']\nluke=['luke','cowboy', 'seagoing','seagoing','cowboys','program','animals']\npollution=['air','pollution','smog','emissions','environment','city','paris','cities']\ntopic_words = venus + mars + cars + electoral +technology + luke + pollution\ntopic_words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_DATA_PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding all words similar to known topic words","metadata":{}},{"cell_type":"code","source":"all_text = ' '.join(df_train['full_text'])\n# Tokenize the text into words, convert to lowercase, and extract unique words\nwords = re.findall(r'\\b\\w+\\b', all_text.lower())\nword_counts = Counter(words)\n# Filter words that occur at least three times\nwords_used_at_least_three_times = [word for word, count in word_counts.items() if count >= 3]\nprint(f'Words used at least three times: {len(words_used_at_least_three_times)}')\nunique_words = set(words_used_at_least_three_times)\nvocab = list(unique_words)\nvocab = sorted(vocab, key=lambda word: word_counts[word], reverse=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get embeddings for topic words\ntopic_embeddings = {word: get_bert_embedding(word, preprocessor, encoder) for word in topic_words}\n\n# as the vocabulary is more than 20 K we need to calculate embeddings in batches\ndef get_bert_embedding(word, preprocessor, encoder):\n    text_input = tf.constant([word])\n    encoder_inputs = preprocessor(text_input)\n    outputs = encoder(encoder_inputs)\n    pooled_output = outputs[\"pooled_output\"]  # [batch_size, 128]\n    return pooled_output.numpy()\n\n# Get embeddings for the vocabulary\ndef process_batch(words):\n    embeddings = {}\n    for word in words:\n        embeddings[word] = get_bert_embedding(word, preprocessor, encoder)\n    return embeddings\n\n# Split the vocabulary into batches\nbatch_size = 1000\nending=len(vocab)\n#for testing\n#ending=500\nbatches = [vocab[i:i+batch_size] for i in range(0, ending, batch_size)]\n\n# Process batches in parallel\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = executor.map(process_batch, batches)\n\n# Combine embeddings from all batches\nvocab_embeddings = {}\nbatch_count = 0\nfor result in results:\n    batch_count += 1\n    print(f\"Processed batch {batch_count} out of {len(batches)}\")\n    vocab_embeddings.update(result)\nprint (2)\n\n# Function to find similar words based on cosine similarity\ndef get_similar_words(topic_embeddings, vocab_embeddings, threshold=0.8):\n    similar_words = set()\n    for topic_word, topic_emb in topic_embeddings.items():\n        for vocab_word, vocab_emb in vocab_embeddings.items():\n            if topic_word != vocab_word:\n                similarity = cosine_similarity(topic_emb, vocab_emb)[0][0]\n                if similarity > threshold:\n                    similar_words.add(vocab_word)\n    return similar_words\n\n# Find similar words (for testing different thresholds)\n#for tr in np.arange(0.98,0.99,0.002):\n#    related_words = get_similar_words(topic_embeddings, vocab_embeddings, threshold=tr)\n#    print(f'{tr} Related Words: {len(related_words)}')\n#    print (related_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"related_words = get_similar_words(topic_embeddings, vocab_embeddings, threshold=0.984)\n# Combine topic words and related words\nall_exclude_words = set(topic_words).union(related_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exclude_words(text, words):\n    pattern = re.compile(r'\\b(' + '|'.join(words) + r')\\b', re.IGNORECASE)\n    return pattern.sub(\"topic\", text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['txt_wth_topic']=df_train['full_text'].apply(lambda x : exclude_words(x,all_exclude_words))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the data\ndf = df_train.sample(frac=1).reset_index(drop=True)\n# Select the columns with the features\ndf_data_x = df[['txt_wth_topic']]\n# Select the column with the label:\ndf_data_y = df['score']\n\n# Split the samples:\nx_train, x_test, y_train, y_test = train_test_split(df_data_x,\n                                                    df_data_y,\n                                                    train_size=TRAIN_SIZE,\n                                                    random_state=42)\n\n# create tokenizer\ntokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token=OOV_TOKEN)\ntokenizer.fit_on_texts(x_train['txt_wth_topic'])  \ntrain_sequences = tokenizer.texts_to_sequences(x_train['txt_wth_topic'])\n\n# padding / truncating\ntrain_padded = pad_sequences(train_sequences,\n                             maxlen=MAX_LENGTH,\n                             padding=PADDING_TYPE,\n                             truncating=TRUNCATION_TYPE)\n\nvalidation_sequences = tokenizer.texts_to_sequences(x_test['txt_wth_topic'])\nvalidation_padded = pad_sequences(validation_sequences,\n                                  maxlen=MAX_LENGTH,\n                                  padding=PADDING_TYPE,\n                                  truncating=TRUNCATION_TYPE)\n\n# encoding\nencoder = LabelEncoder()\nencoder.fit(y_train)    \nencoded_y_train = to_categorical(encoder.transform(y_train))\nencoded_y_test = to_categorical(encoder.transform(y_test))\nnum_classes = len(np.unique(y_train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\ntf.keras.layers.Embedding(VOCABULARY_SIZE, EMBEDDING_DIM),\ntf.keras.layers.Dropout(0.4),\ntf.keras.layers.LSTM(128, return_sequences=True),\ntf.keras.layers.LSTM(64),\ntf.keras.layers.Dense(256, activation=\"relu\"),\ntf.keras.layers.Dense(128, activation=\"relu\"),\ntf.keras.layers.Dropout(0.2),\ntf.keras.layers.Dense(num_classes, activation=\"softmax\")\n]) \n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Build the model by providing the input shape\nmodel.build(input_shape=(None, MAX_LENGTH))\n\n# summary and callback\nmodel.summary()\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n\n# training\nhistory = model.fit(train_padded,\n                    encoded_y_train,\n                    epochs=NUM_EPOCHS,\n                    validation_data=(validation_padded, encoded_y_test),\n                    callbacks=[callback]\n                    ) \n\npredictotron = model.predict(validation_padded) ","metadata":{},"execution_count":null,"outputs":[]}]}